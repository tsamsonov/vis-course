{
  "hash": "3981fc3c6a90430df4e5ed877ff5839b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Классификация одномерных рядов\nsubtitle: Визуализация пространственных данных\ndate: today\ndate-format: long\nauthor: Самсонов Тимофей Евгеньевич\nexecute:\n  echo: false\n  freeze: true\nformat:\n  revealjs:\n    theme:\n      - default\n      - custom.scss\n    margin: 0.2\n    width: 1280\n    height: 720\n    slide-number: true\n    footer: 'Самсонов Т. Е. Визуализация пространственных данных: курс лекций'\n    header-includes: '<link rel=\"stylesheet\" media=\"screen\" href=\"https://fontlibrary.org//face/pt-sans\" type=\"text/css\"/>'\nbibliography: references.yaml\nmainfont: PT Sans\n---\n\n## Функция распределения\n\nФункция распределения $F(x) = P(X < x)$ является одной из основных характеристик случайной величины.\n\nВ практической работе используется выборочная ФР $\\widehat F(x)$.\n\n::: {#e346bb12 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-2-output-1.png){width=995 height=540}\n:::\n:::\n\n\n## Выборочная функция распределения\n\n**Выборочная функция распределения** $\\widehat F(x)$ определяется следующим образом:\n\n$$\n\\widehat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\theta(x-X_i),\n$$\n\nгде $\\theta(x)$ — *функция Хевисайда*:\n\n$$\n\\theta(x)=\\begin{cases} 0, & x<0;\n\\\\ 1, & x\\geqslant 0.\\end{cases}\n$$\n\nПри $n \\rightarrow \\infty$ функция $\\widehat F(x)$ равномерно сходится к $F(x)$ (*теорема Колмогорова*).\n\n::: callout-tip\n## Проще говоря\n\nЧем больше выборка, тем точнее выборочная ФР описывает реальную.\n:::\n\n## Нормальное распределение\n\n::: {#5483116a .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-3-output-1.png){width=995 height=687}\n:::\n:::\n\n\n## Логнормальное распределение\n\n::: {#d5c8dad7 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-4-output-1.png){width=995 height=687}\n:::\n:::\n\n\n## Экспоненциальное распределение\n\n::: {#4c9e4a7a .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-5-output-1.png){width=1008 height=687}\n:::\n:::\n\n\n## Равные интервалы\n\nРазмер класса равен $[\\max(X) - \\min(X)] / k$, где $k$ — число классов.\n\n::: {#1e0a49ad .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-6-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n## Квантили\n\nКоличество элементов в каждом классе $c \\approx n/k$.\n\n::: {#29ec01d7 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-7-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n## Стандартные отклонения\n\nГраницы классов определяются значениями $\\overline X \\pm \\alpha S$, где $\\alpha = 1, 2, ..., (k+1)/2$\n\n::: {#d92eb41f .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-8-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n> Метод подходит только для данных с нормальным распределением.\n\n## Стандартные отклонения\n\nДанный метод не подходит для распределений, отличающихся от нормального. Например, для экспоненциального:\n\n::: {#d8fb170b .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-9-output-1.png){width=1031 height=559}\n:::\n:::\n\n\n> Метод подходит только для данных с нормальным распределением.\n\n## Метод Дженкса-Кэспалла (1971)\n\nГраницы классов определяются путем минимизации суммы расстояний от элементов классов до их медиан (*ADCM* — a*bsolute deviation about class median*):\n\n$$\nADCM = \\sum_{j=1}^k \\sum_{i=1}^{n_k} |X_{ij} - \\operatorname{med}(X_j)|\n$$\n\nМетод базируется на итеративном алгоритме:\n\n1.  Классифицировать выборку методом квантилей на $k$ классов.\n2.  Вычислить для каждого элемента его отклонение от медианы собственного *и соседних* классов.\n3.  Пока есть элементы, располагающиеся ближе к соседней медиане, чем к своей:\n    -   Переместить элемент в ближайший соседний класс\n    -   Обновить значения медиан и расстояний для изменившихся классов и их соседей\n\n## Метод Дженкса-Кэспалла (1971)\n\nГраницы классов определяются путем минимизации ADCM\n\n::: {#fc2e253c .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-10-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n## Метод Фишера-Дженкса (1958, 1977)\n\nТак же как и метод Дженкса-Кэспалла, минимизирует ADCM, однако использует *рекурсивную*, а не итеративную процедуру. В основе лежит теорема:\n\n> Каждое оптимальное разбиение множества есть объединение оптимальных разбиение его подмножеств\n\nТогда границей $j$ оптимального разбиения $X$ на 2 класса будем считать\n\n$$\n\\operatorname{argmin}_j \\big[D(1, j) + D(j+1, n)\\big],\\\\D(a, b) = \\sum_{i = a}^b |X_i - \\operatorname{med}(X_{[a,b]})|\n$$\n\nЕсли число классов $k > 2$, то выбирается тот из них, у которого максимальное значение $D$ и аналогичным образом разбивается на $2$ класса.\n\n## Метод Фишера-Дженкса (1958, 1977)\n\nДанный метод также известен автоматизированный вариант классификации методом *Natural Breaks*\n\n::: {#963e7197 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-11-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n## Метод Head/Tail (Jiang, 2013)\n\nВ данном методе используется рекурсивное разбиение выборки средним значением.\n\n1.  Разбить выборку на $2$ класса, используя среднее значение\n2.  Повторять процедуру для класса выше среднего, до тех пор пока не будет достигнуто\n    -   требуемое число классов или\n    -   доля значений выше среднего не превысит $40\\%$\n\nПороговый критерий в $40\\%$ является экспертным (предложен автором метода) и может быть модифицирован.\n\n> Метод годится для распределений с \"тяжелым хвостом\", т.е. подчиняющихся экспоненциальному закону.\n\n## Метод Head/Tail (Jiang, 2013)\n\nДанный метод плохо работает с нормальным распределением:\n\n::: {#5abf350c .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-12-output-1.png){width=1014 height=559}\n:::\n:::\n\n\n## Метод Head/Tail (Jiang, 2013)\n\nНо хорошо справляется с экспоненциальным:\n\n::: {#4d9dbadb .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-13-output-1.png){width=1031 height=559}\n:::\n:::\n\n\n## GADF \n\n**GADF** **(Goodness of Absolute Deviation Fit)** — метрика, характеризующая эффективность классификации:\n\n$$\nGADF = 1 - \\frac{\\sum_{j=1}^k \\sum_{i=1}^{n_k} |X_{ij} - \\operatorname{med}(X_j)|}{\\sum_{i=1}^{n} |X_{i} - \\operatorname{med}(X)|}\n$$\n\nGADF показывает как отклонения внутри классов соотносятся с отклонениями всей выборки. Чем\n\n::: callout-tip\n## Применение\n\nМетрику GADF можно использовать для того чтобы определить оптимальный метод классификации\n:::\n\n## GADF \n\n**GADF** для **нормального** распределения:\n\n::: {#0ccbd0e9 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-14-output-1.png){width=995 height=603}\n:::\n:::\n\n\n## GADF \n\n**GADF** для **экспоненциального** распределения:\n\n::: {#c8964b3d .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](02_Classifications_files/figure-revealjs/cell-15-output-1.png){width=995 height=603}\n:::\n:::\n\n\n## Библиография\n\n",
    "supporting": [
      "02_Classifications_files"
    ],
    "filters": [],
    "includes": {}
  }
}